{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "fc442234",
      "metadata": {
        "id": "fc442234"
      },
      "source": [
        "# CIFAR-100: Full Lab ‚Äî Account setup, Train ‚Üí W&B ‚Üí Deploy on Hugging Face Spaces\n",
        "**Colab-ready hands-on lab notebook**\n",
        "\n",
        "This notebook combines:\n",
        "- Step-by-step instructions to create a **Hugging Face** account and **access token**,\n",
        "- Secure token input in Colab using `getpass` (token is not printed),\n",
        "- **W&B** authentication,\n",
        "- Training a `resnet18` on **CIFAR-100**, logging to W&B and saving model as an artifact,\n",
        "- A Gradio app (`app.py`) that downloads the model artifact from W&B at startup,\n",
        "- A script to create & push a **Hugging Face Space** (Gradio) from Colab using `huggingface_hub`.\n",
        "\n",
        "**Security note:** Never commit your tokens/API keys to public repos. Use Colab `getpass` to keep tokens secret and use HF Space Secrets for W&B keys.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "f4ac50e3",
      "metadata": {
        "id": "f4ac50e3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d9704063-d5c0-48d0-8e8d-d00eceed5d00"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---- Environment Validation ----\n",
            "GPU available: True\n",
            "GPU name: Tesla T4\n",
            "HF_TOKEN: NOT SET\n",
            "HF_USER: NOT SET\n",
            "SPACE_NAME: NOT SET\n",
            "WANDB_API_KEY: ‚ö†Ô∏è Not set (you'll log in interactively)\n",
            "Dependencies:  wandb, gradio, huggingface_hub imported successfully\n",
            "---------------------------------\n"
          ]
        }
      ],
      "source": [
        "#@title üîç Environment Validation (run before starting lab)\n",
        "import torch, os\n",
        "\n",
        "print(\"---- Environment Validation ----\")\n",
        "\n",
        "# Check GPU availability\n",
        "print(\"GPU available:\", torch.cuda.is_available())\n",
        "if torch.cuda.is_available():\n",
        "    print(\"GPU name:\", torch.cuda.get_device_name(0))\n",
        "\n",
        "# Check required env vars\n",
        "for var in [\"HF_TOKEN\", \"HF_USER\", \"SPACE_NAME\"]:\n",
        "    val = os.environ.get(var)\n",
        "    print(f\"{var}:\", \" SET\" if val else \"NOT SET\")\n",
        "\n",
        "# Check if W&B API key configured (login handled separately)\n",
        "wandb_key = os.environ.get(\"WANDB_API_KEY\")\n",
        "print(\"WANDB_API_KEY:\", \" SET\" if wandb_key else \"‚ö†Ô∏è Not set (you'll log in interactively)\")\n",
        "\n",
        "# Check if dependencies installed\n",
        "try:\n",
        "    import wandb, gradio, huggingface_hub\n",
        "    print(\"Dependencies:  wandb, gradio, huggingface_hub imported successfully\")\n",
        "except Exception as e:\n",
        "    print(\"Dependencies: missing - please run install cell first\")\n",
        "    print(e)\n",
        "\n",
        "print(\"---------------------------------\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "5fe559f7",
      "metadata": {
        "id": "5fe559f7"
      },
      "outputs": [],
      "source": [
        "#@title 1) Install dependencies (run once)\n",
        "# Installs pytorch (with CUDA if available), W&B, Gradio and HF tooling.\n",
        "!pip install -q torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/cu118\n",
        "!pip install -q wandb gradio huggingface_hub git-lfs\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7ded0f9e",
      "metadata": {
        "id": "7ded0f9e"
      },
      "source": [
        "## 2) Hugging Face account & token (student instructions)\n",
        "\n",
        "**If you don't have a Hugging Face account:**\n",
        "1. Go to https://huggingface.co and click **Sign up**. Verify email.\n",
        "2. Choose a username (used as `HF_USER`).\n",
        "\n",
        "**Create an Access Token:**\n",
        "1. Go to https://huggingface.co/settings/tokens\n",
        "2. Click **New token**. Name it (e.g., `colab-space-token`) and select scope `repo` (read & write).\n",
        "3. Click **Create** and copy the token now ‚Äî you won't see it again.\n",
        "\n",
        "**Important:** Keep the token secret. Use the next cell to set it securely in Colab (the token will not be printed)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "399c2879",
      "metadata": {
        "id": "399c2879",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9ae12a70-b98e-422f-aefc-6f37b06a94c8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Paste your Hugging Face token when prompted. It will be hidden.\n",
            "Hugging Face token: ¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑\n",
            "Enter your Hugging Face username (e.g. 'alice'): vaibhav9029\n",
            "Enter desired Space name (e.g. 'cifar100-demo-space'): cifar100-demo-space\n",
            "HF_TOKEN stored in runtime (hidden). HF_USER and SPACE_NAME saved in environment variables.\n"
          ]
        }
      ],
      "source": [
        "#@title 3) Securely set your Hugging Face token, username, and desired Space name\n",
        "from getpass import getpass\n",
        "import os\n",
        "\n",
        "print(\"Paste your Hugging Face token when prompted. It will be hidden.\")\n",
        "hf_token = getpass(\"Hugging Face token: \")\n",
        "os.environ['HF_TOKEN'] = hf_token\n",
        "\n",
        "# Edit these values (do NOT put the token here)\n",
        "hf_user = input(\"Enter your Hugging Face username (e.g. 'alice'): \").strip()\n",
        "space_name = input(\"Enter desired Space name (e.g. 'cifar100-demo-space'): \").strip()\n",
        "\n",
        "os.environ['HF_USER'] = hf_user\n",
        "os.environ['SPACE_NAME'] = space_name\n",
        "\n",
        "print(\"HF_TOKEN stored in runtime (hidden). HF_USER and SPACE_NAME saved in environment variables.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "a4723f4e",
      "metadata": {
        "id": "a4723f4e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "224a655c-99ef-4e84-ae4e-d7d0c208b4f4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Follow the prompt to authenticate W&B (this opens an input box).\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "#@title 4) Authenticate Weights & Biases (W&B)\n",
        "import wandb\n",
        "print(\"Follow the prompt to authenticate W&B (this opens an input box).\")\n",
        "wandb.login()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "168998a0",
      "metadata": {
        "id": "168998a0"
      },
      "source": [
        "## 5) Create `train.py` ‚Äî training + W&B artifact logging\n",
        "\n",
        "This script trains ResNet18 on CIFAR-100, logs metrics to W&B, and saves the best model as a W&B artifact.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "id": "a5635d6e",
      "metadata": {
        "id": "a5635d6e"
      },
      "outputs": [],
      "source": [
        "%%bash\n",
        "cat > train.py <<'PY'\n",
        "import argparse\n",
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import wandb\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision.models import resnet18\n",
        "print(\"god\")\n",
        "def parse_args():\n",
        "    p = argparse.ArgumentParser()\n",
        "    p.add_argument(\"--project\", type=str, default=\"cifar100-hf-demo\")\n",
        "    p.add_argument(\"--entity\", type=str, default=None)\n",
        "    p.add_argument(\"--epochs\", type=int, default=5)\n",
        "    p.add_argument(\"--batch-size\", type=int, default=128)\n",
        "    p.add_argument(\"--lr\", type=float, default=0.01)\n",
        "    return p.parse_args()\n",
        "\n",
        "def get_dataloaders(batch_size):\n",
        "    transform_train = transforms.Compose([\n",
        "        transforms.RandomCrop(32, padding=4),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.5071, 0.4865, 0.4409),\n",
        "                             (0.2673, 0.2564, 0.2762)),\n",
        "    ])\n",
        "    transform_test = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.5071, 0.4865, 0.4409),\n",
        "                             (0.2673, 0.2564, 0.2762)),\n",
        "    ])\n",
        "    trainset = torchvision.datasets.CIFAR100(root=\"./data\", train=True, download=True, transform=transform_train)\n",
        "    testset  = torchvision.datasets.CIFAR100(root=\"./data\", train=False, download=True, transform=transform_test)\n",
        "    trainloader = DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
        "    testloader  = DataLoader(testset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
        "    return trainloader, testloader\n",
        "\n",
        "def train_one_epoch(model, device, loader, optimizer, criterion):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for i, (inputs, targets) in enumerate(loader):\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, targets)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item() * inputs.size(0)\n",
        "        _, predicted = outputs.max(1)\n",
        "        total += targets.size(0)\n",
        "        correct += predicted.eq(targets).sum().item()\n",
        "    return running_loss / total, 100. * correct / total\n",
        "\n",
        "def evaluate(model, device, loader, criterion):\n",
        "    model.eval()\n",
        "    loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for inputs, targets in loader:\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "            outputs = model(inputs)\n",
        "            l = criterion(outputs, targets)\n",
        "            loss += l.item() * inputs.size(0)\n",
        "            _, predicted = outputs.max(1)\n",
        "            total += targets.size(0)\n",
        "            correct += predicted.eq(targets).sum().item()\n",
        "    return loss/total, 100.*correct/total\n",
        "\n",
        "def main():\n",
        "    args = parse_args()\n",
        "    wandb.init(project=args.project, entity=args.entity, config=vars(args))\n",
        "    cfg = wandb.config\n",
        "\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    trainloader, testloader = get_dataloaders(cfg.batch_size)\n",
        "\n",
        "    model = resnet18(num_classes=100)\n",
        "    model = model.to(device)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.SGD(model.parameters(), lr=cfg.lr, momentum=0.9, weight_decay=5e-4)\n",
        "\n",
        "    best_acc = 0.0\n",
        "    for epoch in range(cfg.epochs):\n",
        "        train_loss, train_acc = train_one_epoch(model, device, trainloader, optimizer, criterion)\n",
        "        test_loss, test_acc = evaluate(model, device, testloader, criterion)\n",
        "        wandb.log({\"epoch\": epoch+1, \"train_loss\": train_loss, \"train_acc\": train_acc,\n",
        "                   \"test_loss\": test_loss, \"test_acc\": test_acc})\n",
        "        print(f\"Epoch {epoch+1}: train_acc={train_acc:.2f} test_acc={test_acc:.2f}\")\n",
        "        if test_acc > best_acc:\n",
        "            best_acc = test_acc\n",
        "            os.makedirs(\"outputs\", exist_ok=True)\n",
        "            torch.save(model.state_dict(), \"outputs/model.pt\")\n",
        "            # log artifact\n",
        "            artifact = wandb.Artifact(\"resnet18-cifar100\", type=\"model\", metadata={\"test_acc\": best_acc})\n",
        "            artifact.add_file(\"outputs/model.pt\")\n",
        "            wandb.log_artifact(artifact)\n",
        "    print(\"Best test acc:\", best_acc)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n",
        "PY\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wandb login --relogin"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FB-aGXySB5NO",
        "outputId": "8d520025-8bb1-4c6c-ebba-66f21fa7d0a7"
      },
      "id": "FB-aGXySB5NO",
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize?ref=models\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: W&B API key is configured. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "e2398c34",
      "metadata": {
        "id": "e2398c34",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5f17fa01-2cc9-4ea2-c581-beae2ebb37c3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33m142502033\u001b[0m (\u001b[33mir2023\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚¢ø\u001b[0m Waiting for wandb.init()...\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.22.2\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/wandb/run-20251021_043557-pe4565kz\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mlunar-blaze-3\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at \u001b[34m\u001b[4mhttps://wandb.ai/ir2023/cifar100-hf-demo\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run at \u001b[34m\u001b[4mhttps://wandb.ai/ir2023/cifar100-hf-demo/runs/pe4565kz\u001b[0m\n",
            "100% 169M/169M [00:04<00:00, 41.5MB/s]\n",
            "Epoch 1: train_acc=10.56 test_acc=17.55\n",
            "Epoch 2: train_acc=19.53 test_acc=24.56\n",
            "Epoch 3: train_acc=24.89 test_acc=28.80\n",
            "Best test acc: 28.8\n",
            "\u001b[1;34mwandb\u001b[0m: \n",
            "\u001b[1;34mwandb\u001b[0m: üöÄ View run \u001b[33mlunar-blaze-3\u001b[0m at: \u001b[34mhttps://wandb.ai/ir2023/cifar100-hf-demo/runs/pe4565kz\u001b[0m\n",
            "\u001b[1;34mwandb\u001b[0m: Find logs at: \u001b[1;35mwandb/run-20251021_043557-pe4565kz/logs\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "#@title 6) Run training (edit the --entity to your W&B username/team)\n",
        "# Keep epochs small for demo (e.g., 3-5). Increase for better accuracy.\n",
        "!python train.py --project cifar100-hf-demo --entity ir2023 --epochs 3 --batch-size 128\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f0db3cd7",
      "metadata": {
        "id": "f0db3cd7"
      },
      "source": [
        "## 7) Download model artifact from W&B (if you didn't run training here)\n",
        "This downloads the latest logged artifact into `outputs/` so the Gradio app can load it."
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "i_bcgzljuC7y"
      },
      "id": "i_bcgzljuC7y",
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "ed2479c6",
      "metadata": {
        "id": "ed2479c6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0f127b9b-8cee-4b05-95d0-786a784708d2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloaded artifact to outputs/\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "wandb:   1 of 1 files downloaded.  \n"
          ]
        }
      ],
      "source": [
        "%%bash\n",
        "python - <<'PY'\n",
        "import wandb, os, sys\n",
        "ENTITY = os.environ.get(\"WANDB_ENTITY\") or \"ir2023\"   # <-- edit if not set\n",
        "PROJECT = \"cifar100-hf-demo\"\n",
        "ARTIFACT = \"resnet18-cifar100:latest\"\n",
        "api = wandb.Api()\n",
        "try:\n",
        "    artifact = api.artifact(f\"{ENTITY}/{PROJECT}/{ARTIFACT}\")\n",
        "    artifact.download(root=\"outputs\")\n",
        "    print(\"Downloaded artifact to outputs/\")\n",
        "except Exception as e:\n",
        "    print(\"Failed to download artifact:\", e)\n",
        "    sys.exit(1)\n",
        "PY\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ab2f6d6a",
      "metadata": {
        "id": "ab2f6d6a"
      },
      "source": [
        "## 8) Create Gradio app (`app.py`) that downloads the model from W&B at startup\n",
        "\n",
        "This approach avoids committing large model files into the Space repository. The Space must have `WANDB_API_KEY` set as a secret in its settings.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "id": "08154d69",
      "metadata": {
        "id": "08154d69",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 646
        },
        "outputId": "a8b1c20c-420e-4937-87e9-c81301d1a95d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "It looks like you are running Gradio on a hosted Jupyter notebook, which requires `share=True`. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://cad6d603423269ee93.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://cad6d603423269ee93.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "import os, time, io\n",
        "from PIL import Image\n",
        "import torch\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.models import resnet18\n",
        "import gradio as gr\n",
        "\n",
        "MODEL_PATH = \"outputs/model.pt\"\n",
        "\n",
        "# If model not present, try download via W&B (requires WANDB_API_KEY secret in Space or env)\n",
        "if not os.path.exists(MODEL_PATH):\n",
        "\n",
        "    try:\n",
        "        import wandb\n",
        "        wandb_api_key = os.environ.get(\"WANDB_API_KEY\")\n",
        "        if wandb_api_key:\n",
        "            wandb.login(key=wandb_api_key)\n",
        "            api = wandb.Api()\n",
        "            artifact = api.artifact(os.environ.get(\"WANDB_ARTIFACT\", \"ir2023/cifar100-hf-demo/resnet18-cifar100:v12\"))\n",
        "            artifact.download(root=\"outputs\")\n",
        "            print(\"Downloaded model via W&B artifact.\")\n",
        "        else:\n",
        "            print(\"WANDB_API_KEY not set; cannot download artifact.\")\n",
        "    except Exception as e:\n",
        "        print(\"Error downloading artifact via W&B:\", e)\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model = resnet18(num_classes=100)\n",
        "model.load_state_dict(torch.load(MODEL_PATH, map_location=device))\n",
        "model.to(device)\n",
        "model.eval()\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((32,32)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5071, 0.4865, 0.4409),(0.2673,0.2564,0.2762))\n",
        "])\n",
        "\n",
        "def predict_image(img):\n",
        "    start = time.time()\n",
        "    x = transform(img).unsqueeze(0).to(device)\n",
        "    with torch.no_grad():\n",
        "        out = model(x)\n",
        "        probs = torch.nn.functional.softmax(out, dim=1)\n",
        "        conf, idx = probs.max(1)\n",
        "        class_idx = int(idx.item())\n",
        "        conf_val = float(conf.item())\n",
        "    latency = (time.time() - start) * 1000.0\n",
        "    return {\"class_idx\": class_idx, \"confidence\": round(conf_val,4), \"latency_ms\": round(latency,2)}\n",
        "\n",
        "iface = gr.Interface(fn=predict_image, inputs=gr.Image(type=\"pil\"), outputs=\"json\", title=\"CIFAR-100 demo\")\n",
        "if __name__ == \"__main__\":\n",
        "    iface.launch(server_name=\"0.0.0.0\", server_port=7860)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "id": "4637e6d4",
      "metadata": {
        "id": "4637e6d4"
      },
      "outputs": [],
      "source": [
        "%%bash\n",
        "cat > requirements.txt <<'REQ'\n",
        "torch\n",
        "torchvision\n",
        "gradio\n",
        "Pillow\n",
        "wandb\n",
        "huggingface_hub\n",
        "git-lfs\n",
        "REQ\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "76dd2e1f",
      "metadata": {
        "id": "76dd2e1f"
      },
      "source": [
        "## 9) Create & push a Hugging Face Space from Colab (uses HF_TOKEN set earlier via getpass)\n",
        "\n",
        "This will:\n",
        "- create the Space repo (if it doesn't exist),\n",
        "- push `app.py` and `requirements.txt` to the Space,\n",
        "- not include the heavy model file (the app downloads it from W&B at runtime).\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GiJghsx9xrb6"
      },
      "id": "GiJghsx9xrb6",
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "id": "7afe27a0",
      "metadata": {
        "id": "7afe27a0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b20ca21a-70e8-4c8a-9e89-18dcdb883d00"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initialized empty Git repository in /content/hf_space/.git/\n",
            "Updated git hooks.\n",
            "Git LFS initialized.\n",
            "Repo URL: https://huggingface.co/spaces/vaibhav9029/cifar100-demo-space\n",
            "Pushed to: https://huggingface.co/spaces/vaibhav9029/cifar100-demo-space\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "hint: Using 'master' as the name for the initial branch. This default branch name\n",
            "hint: is subject to change. To configure the initial branch name to use in all\n",
            "hint: of your new repositories, which will suppress this warning, call:\n",
            "hint: \n",
            "hint: \tgit config --global init.defaultBranch <name>\n",
            "hint: \n",
            "hint: Names commonly chosen instead of 'master' are 'main', 'trunk' and\n",
            "hint: 'development'. The just-created branch can be renamed via this command:\n",
            "hint: \n",
            "hint: \tgit branch -m <name>\n"
          ]
        }
      ],
      "source": [
        "%%bash\n",
        "set -e\n",
        "# prepare local repo\n",
        "rm -rf hf_space || true\n",
        "mkdir hf_space\n",
        "cp app.py requirements.txt hf_space/\n",
        "cd hf_space\n",
        "\n",
        "git init\n",
        "git config user.email \"142502033@smail.iitpkd.ac.in\"\n",
        "git config user.name \"Vaibhav9029\"\n",
        "git lfs install\n",
        "\n",
        "python - <<'PY'\n",
        "from huggingface_hub import HfApi, Repository\n",
        "import os, sys\n",
        "token = os.environ.get(\"HF_TOKEN\")\n",
        "user = os.environ.get(\"HF_USER\")\n",
        "space = os.environ.get(\"SPACE_NAME\")\n",
        "if not token or not user or not space:\n",
        "    print(\"HF_TOKEN, HF_USER or SPACE_NAME not set. Aborting.\")\n",
        "    sys.exit(1)\n",
        "api = HfApi(token=token)\n",
        "\n",
        "repo_id = f\"{user}/{space}\"\n",
        "repo_url = api.create_repo(repo_id=repo_id, repo_type=\"space\",\n",
        "            space_sdk=\"gradio\",\n",
        "            exist_ok=True)\n",
        "print(\"Repo URL:\", repo_url)\n",
        "\n",
        "api.upload_folder(\n",
        "    folder_path=\".\",\n",
        "    repo_id=repo_id,\n",
        "    repo_type=\"space\",\n",
        "    commit_message=\"Initial commit: CIFAR-100 Gradio app (no model)\"\n",
        ")\n",
        "\n",
        "print(\"Pushed to:\", repo_url)\n",
        "PY\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0a5b107f",
      "metadata": {
        "id": "0a5b107f"
      },
      "source": [
        "## 10) Adding W&B API key as a secret in the Hugging Face Space\n",
        "\n",
        "After pushing the Space, do:\n",
        "1. Visit `https://huggingface.co/spaces/<HF_USER>/<SPACE_NAME>`.\n",
        "2. Click **Settings ‚Üí Secrets**.\n",
        "3. Add a secret named `WANDB_API_KEY` and paste your W&B API key (keep secret).\n",
        "4. Optionally set `WANDB_ARTIFACT` to the artifact path, e.g. `your_wandb_entity/cifar100-hf-demo/resnet18-cifar100:latest`.\n",
        "5. Rebuild the Space (there's a \"Rebuild\" button) so the app can download the artifact at startup.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6e7cde70",
      "metadata": {
        "id": "6e7cde70"
      },
      "source": [
        "## 11) Test the deployed Space\n",
        "Open the Space URL `https://huggingface.co/spaces/<HF_USER>/<SPACE_NAME>`.\n",
        "- Check the Space logs (Settings ‚Üí Logs) if the model download or runtime fails.\n",
        "- Common issues: missing secret, wrong artifact name, package install errors (pin versions in requirements.txt).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0b323968",
      "metadata": {
        "id": "0b323968"
      },
      "source": [
        "## 12) Instructor tips and cleanup\n",
        "- Keep epochs small during demos.\n",
        "- Instructor can host one shared Space and W&B project to avoid per-student pushes.\n",
        "- After course, remove the Space or set it to private if not needed.\n",
        "- Revoke tokens if accidentally exposed: Hugging Face (Settings ‚Üí Access Tokens), W&B (Settings ‚Üí API Keys).\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FTLB0C6ZKOrT"
      },
      "id": "FTLB0C6ZKOrT",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}